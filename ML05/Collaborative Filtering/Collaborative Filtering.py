# -*- coding: utf-8 -*-
"""CollaborativeFiltering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16L3Aw13HjyLz3usFabyyZMLvsZq3oSta

### Tập dữ liệu sử dụng là ml-latest-small bao gồm: 100,000 ratings and 3,600 tag applications applied to 9,000 movies by 600 users. Last updated 9/2018.
"""

!https://files.grouplens.org/datasets/movielens/ml-latest-small.zip

"""# **Collaboratvie Filtering (User-User)** (Chạy được)

**Thư viện + Đọc dữ liệu**
"""

import pandas as pd
import numpy as np

# Đọc dữ liệu từ tập dữ liệu ml-latest-small
ratings = pd.read_csv('/content/drive/MyDrive/ml-latest-small/ratings.csv')
movies = pd.read_csv('/content/drive/MyDrive/ml-latest-small/movies.csv')

ratings_table = ratings.pivot(index='userId', columns='movieId', values='rating')
ratings_table.to_csv('/content/drive/MyDrive/ml-latest-small/ratings_table.csv', index=True)

ratings_table

"""**Chuẩn hóa dữ liệu, và thay thế các giá trị NaN bằng giá trị trung bình**"""

# Tính điểm trung bình của từng người dùng cho tất cả các phim họ đã đánh giá
user_avg_ratings = ratings_table.mean(axis=1)

# Chuẩn hóa DataFrame bằng cách trừ giá trị trung bình và thay thế NaN bằng giá trị trung bình
normalized_ratings_tables = ratings_table.sub(user_avg_ratings, axis=0)

#a = normalized_ratings_tables.mean(axis=0)
normalized_ratings_table = normalized_ratings_tables.fillna(0)

normalized_ratings_table

"""**Hàm lấy các MovieId mà User_to_recommend chưa rate**"""

#Lấy Index (movieId) các bộ phim mà User_to_recommend chưa cho điểm ratings
def unrated_movie_index(User_to_recommend):
    #Chọn dòng tương ứng với User_to_recommend từ ratings_table
    user_ratings = ratings_table.loc[User_to_recommend]

    #Lấy chỉ mục (movieId) của các giá trị NaN trong dòng đã chọn
    nan_movie_ids = user_ratings[user_ratings.isna()].index

    #Tạo DataFrame mới với chỉ mục là các movieId NaN và các cột là các giá trị NaN tương ứng
    nan_ratings_df = pd.DataFrame(index=nan_movie_ids, columns=['Rating'])

    return nan_ratings_df

"""**Xây dựng ma trận tương quan User-User**"""

from sklearn.metrics.pairwise import cosine_similarity

# Tính ma trận tương quan giữa các người dùng bằng Cosine Similarity
user_similarity_matrix = cosine_similarity(normalized_ratings_table)

# Chuyển ma trận tương quan thành DataFrame
user_similarity_df = pd.DataFrame(user_similarity_matrix, index=normalized_ratings_table.index, columns=normalized_ratings_table.index)

# Đặt giá trị UserId-UserId thành 0
np.fill_diagonal(user_similarity_matrix, 0)

user_similarity_df

"""**Hàm**"""

#Lấy các giá trị UserId đã thực sự vote cho movie_id tương ứng.
def get_user_ids_list(movie_id):
    # Lấy cột tương ứng với movie_id từ ratings_table
    movie_ratings = ratings_table[movie_id]

    # Lọc ra các giá trị không phải NaN
    non_nan_ratings = movie_ratings.dropna()

    # Tạo DataFrame chứa giá trị rating và index (user id) tương ứng
    user_ratings_df = pd.DataFrame({'user_id': non_nan_ratings.index, 'rating': non_nan_ratings.values})

    # Lấy danh sách user_ids
    user_ids_list = user_ratings_df['user_id'].to_list()

    return user_ids_list

#Tạo DataFrame các giá trị Similarity của User_to_recommend với tất cả các User.
def get_sorted_user_similarity(User_to_recommend):
    # Lấy dòng tương ứng với User_to_recommend từ user_similarity_df
    user_similarity_row = user_similarity_df.loc[User_to_recommend]

    # Tạo DataFrame mới chứa các giá trị tương quan và user_id tương ứng
    user_similarity_values_df = pd.DataFrame({'user_id': user_similarity_df.index, 'similarity': user_similarity_row.values})

    # Loại bỏ User_to_recommend từ DataFrame
    user_similarity_values_df = user_similarity_values_df[user_similarity_values_df['user_id'] != User_to_recommend]

    # Sắp xếp theo giảm dần theo giá trị tương quan
    user_similarity_values_df = user_similarity_values_df.sort_values(by='similarity', ascending=False)

    return user_similarity_values_df

#Ghép các cột giá tri cần thiết bao gồm normalized_rating và user_id
def get_sorted_merged_df_filtered(movie_id, user_similarity_values_df, normalized_ratings_table, user_ids_list):
    # Lấy cột tương ứng với movie_id từ normalized_ratings_table
    normalized_ratings = normalized_ratings_table[movie_id]

    # Tạo DataFrame mới chứa giá trị normalized và user_id tương ứng
    normalized_ratings_df = pd.DataFrame({'user_id': normalized_ratings.index, 'normalized_rating': normalized_ratings.values})

    # Ghép user_similarity_values_df và normalized_ratings_df theo cột 'user_id'
    merged_df = pd.merge(user_similarity_values_df, normalized_ratings_df, on='user_id')

    # Loại bỏ các hàng không có trong user_ids_list
    sorted_merged_df_filtered = merged_df[merged_df['user_id'].isin(user_ids_list)]

    return sorted_merged_df_filtered

#Sắp xếp và hiển thị giá trị predicted tương ứng đối với N bộ phim cho User_to_recommend
def create_top_movies_with_title(predicted_ratings_library, movies, N):
    # Chuyển thư viện thành DataFrame
    predicted_ratings_df = pd.DataFrame(predicted_ratings_library)

    # Lấy N phim có predicted rating cao nhất
    top_movies = predicted_ratings_df.nlargest(N, 'predicted_rating')

    # Kết hợp thông tin từ DataFrame movies vào top_movies
    top_movies_with_title = pd.merge(top_movies, movies[['movieId', 'title']], left_on='movie_id', right_on='movieId')

    # Loại bỏ cột 'movieId'
    top_movies_with_title = top_movies_with_title.drop(columns=['movieId'])

    # Di chuyển cột 'predicted_rating' sang vị trí thứ 3
    top_movies_with_title = top_movies_with_title[['movie_id', 'title', 'predicted_rating']]

    return top_movies_with_title

def predicted(User_to_recommend):

    predicted_ratings_library = []

    for movie_id in unrated_movie_index(User_to_recommend).index:

        #Lấy các giá trị UserId đã thực sự vote cho movie_id tương ứng
        a = get_user_ids_list(movie_id)

        #Tạo DataFrame các giá trị Similarity của User_to_recommend với tất cả các User.
        b = get_sorted_user_similarity(User_to_recommend)

        #Ghép các cột giá tri cần thiết bao gồm normalized_rating và user_id
        c = get_sorted_merged_df_filtered(movie_id, b, normalized_ratings_table, a)

        # Lấy 5 giá trị đầu tiên của sorted_merged_df_filtered
        top_users = c.head(5)

        # Tính giá trị predicted_ratings theo công thức
        predicted_ratings = (top_users['similarity'] * top_users['normalized_rating']).sum() / abs(top_users['normalized_rating']).sum()

        # Chuẩn hóa lại giá trị predicted_ratings
        predicted_ratings = user_avg_ratings[User_to_recommend] + predicted_ratings

        # Ghi nhận thông tin vào thư viện
        predicted_ratings_library.append({'movie_id': movie_id, 'predicted_rating': predicted_ratings})

    #Sắp xếp và hiển thị giá trị predicted tương ứng đối với N bộ phim cho User_to_recommend
    result = create_top_movies_with_title(predicted_ratings_library, movies, 5)

    return result

# Chọn một userId cụ thể để recommend
User_to_recommend = 50
predicted(User_to_recommend)

"""# **LightFM Library**"""

pip install lightfm

import numpy as np
import pandas as pd
from lightfm import LightFM
from lightfm.data import Dataset
from lightfm.evaluation import precision_at_k

# Đường dẫn đến tập dữ liệu MovieLens ml-latest-small
movies_path = '/content/drive/MyDrive/ml-latest-small/movies.csv'
ratings_path = '/content/drive/MyDrive/ml-latest-small/ratings.csv'

# Đọc tập dữ liệu movies.csv
movies_df = pd.read_csv(movies_path)

# Đọc tập dữ liệu ratings.csv
ratings_df = pd.read_csv(ratings_path)

# Tạo đối tượng Dataset từ dữ liệu ratings và ma trận đặc trưng của các mục
dataset = Dataset()
dataset.fit(users=ratings_df['userId'], items=movies_df['movieId'], item_features=movies_df['genres'])

# Xây dựng ma trận đánh giá từ dữ liệu ratings
(interactions, _) = dataset.build_interactions([(row['userId'], row['movieId'], row['rating']) for _, row in ratings_df.iterrows()])

# Xây dựng ma trận đặc trưng của các mục từ dữ liệu genres
item_features = dataset.build_item_features(((row['movieId'], [row['genres']]) for _, row in movies_df.iterrows()))

# Kiểm tra số lượng hàng trong ma trận đặc trưng
num_items = dataset.item_features_shape()[0]

# Tạo mô hình LightFM sử dụng phương pháp Content-Based Filtering (CB)
model = LightFM(loss='warp', item_alpha=0.1)  # Sử dụng loss function WARP và item regularization

# Huấn luyện mô hình
model.fit(interactions, item_features=item_features, epochs=10, num_threads=2)

# Đưa ra gợi ý cho người dùng
user_id = 50

# Đưa ra xếp hạng dự đoán cho người dùng
predicted_scores = model.predict(user_id, np.arange(num_items), item_features=item_features)

# Sắp xếp xếp hạng dự đoán từ cao đến thấp
top_items = np.argsort(-predicted_scores)

# In ra 10 phim được đề xuất cho người dùng
top_movies = movies_df[movies_df['movieId'].isin(top_items[:10])]
top_movies[['movieId', 'title']]

"""# **Hybrid Filtering**"""

import pandas as pd
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.metrics import pairwise_distances

movies = pd.read_csv("/content/drive/MyDrive/ml-latest-small/movies.csv",encoding="Latin1")
Ratings = pd.read_csv("/content/drive/MyDrive/ml-latest-small/ratings.csv")
Tags = pd.read_csv("/content/drive/MyDrive/ml-latest-small/tags.csv",encoding="Latin1")

# Tính điểm trung bình của từng người dùng cho tất cả các phim họ đã đánh giá
Mean = Ratings.groupby(by="userId", as_index=False)['rating'].mean()

#ghép cột và trừ đi giá trị trung bình Mean để ra cột adg_rating
Rating_avg = pd.merge(Ratings, Mean, on='userId')
Rating_avg['adg_rating'] = Rating_avg['rating_x'] - Rating_avg['rating_y']
Rating_avg

#Dựng bảng ratings_table như trên
check = pd.pivot_table(Rating_avg,values='rating_x',index='userId',columns='movieId')

#Cũng là ratings_table nhưng đã chuẩn hóa
final = pd.pivot_table(Rating_avg,values='adg_rating',index='userId',columns='movieId')

#Thay thế các giá trị NaN trong final bằng giá trị trung bình Mean của từng cột (tức từng movieID tương ứng)
final_movie = final.fillna(final.mean(axis=0))
final_movie.head()

#Thay thế giá trị NaN bằng giá trị trung bình của hàng theo hàng (tức từng UserId)
final_user = final.T.fillna(final.mean(axis=1)).T
final_user.head()

#Tính độ tương đồng Cosine giữa các movie với nhau
cosine = cosine_similarity(final_movie)
np.fill_diagonal(cosine, 0)
similarity_with_movie = pd.DataFrame(cosine, index=final_movie.index)
similarity_with_movie.columns = final_user.index
similarity_with_movie

# user similarity on replacing NAN by user avg
b = cosine_similarity(final_user)
np.fill_diagonal(b, 0 )
similarity_with_user = pd.DataFrame(b,index=final_user.index)
similarity_with_user.columns=final_user.index
similarity_with_user.head()

def get_user_similar_movies( user1, user2 ):

    common_movies = Rating_avg[Rating_avg.userId == user1].merge(
    Rating_avg[Rating_avg.userId == user2], on = "movieId", how = "inner" )

    return common_movies.merge(movies, on = 'movieId' )

a = get_user_similar_movies(370,86309)
a = a.loc[ : , ['rating_x_x','rating_x_y','title']]
print(a.head())

def find_n_neighbours(df,n):
    order = np.argsort(df.values, axis=1)[:, :n]
    df = df.apply(lambda x: pd.Series(x.sort_values(ascending=False)
           .iloc[:n].index,
          index=['top{}'.format(i) for i in range(1, n+1)]), axis=1)
    return df

# top 30 neighbours for each user
sim_user_10_u = find_n_neighbours(similarity_with_user,10)
print(sim_user_10_u.head())

sim_user_10_m = find_n_neighbours(similarity_with_movie,10)
print(sim_user_10_m.head())

def User_item_score(user,item):
    a = sim_user_10_m[sim_user_10_m.index==user].values
    b = a.squeeze().tolist()
    c = final_movie.loc[:,item]
    d = c[c.index.isin(b)]
    f = d[d.notnull()]
    avg_user = Mean.loc[Mean['userId'] == user,'rating'].values[0]
    index = f.index.values.squeeze().tolist()
    corr = similarity_with_movie.loc[user,index]
    fin = pd.concat([f, corr], axis=1)
    fin.columns = ['adg_score','correlation']
    fin['score']=fin.apply(lambda x:x['adg_score'] * x['correlation'],axis=1)
    nume = fin['score'].sum()
    deno = fin['correlation'].sum()
    final_score = avg_user + (nume/deno)
    return final_score

score = User_item_score(50,4064)
print("score (u,i) is",score)

Rating_avg = Rating_avg.astype({"movieId": str})
Movie_user = Rating_avg.groupby(by='userId')['movieId'].apply(lambda x: ','.join(x))

def User_item_score1(user):
    Movie_seen_by_user = check.columns[check[check.index == user].notna().any()].tolist()
    a = sim_user_10_m[sim_user_10_m.index == user].values
    b = a.squeeze().tolist()
    d = Movie_user[Movie_user.index.isin(b)]
    l = ','.join(d.values)
    Movie_seen_by_similar_users = l.split(',')
    Movies_under_consideration = list(set(Movie_seen_by_similar_users) - set(list(map(str, Movie_seen_by_user))))
    Movies_under_consideration = list(map(int, Movies_under_consideration))
    score = []
    for item in Movies_under_consideration:
        c = final_movie.loc[:, item]
        d = c[c.index.isin(b)]
        f = d[d.notnull()]
        avg_user = Mean.loc[Mean['userId'] == user, 'rating'].values[0]
        index = f.index.values.squeeze().tolist()
        corr = similarity_with_movie.loc[user, index]
        fin = pd.concat([f, corr], axis=1)
        fin.columns = ['adg_score', 'correlation']
        fin['score'] = fin.apply(lambda x: x['adg_score'] * x['correlation'], axis=1)
        nume = fin['score'].sum()
        deno = fin['correlation'].sum()
        final_score = avg_user + (nume / deno)
        score.append(final_score)
    data = pd.DataFrame({'movieId': Movies_under_consideration, 'score': score})
    top_5_recommendation = data.sort_values(by='score', ascending=False).head(5)
    Movie_Name = top_5_recommendation.merge(movies, how='inner', on='movieId')
    Movie_Names = Movie_Name.title.values.tolist()
    return Movie_Names

user = input("Enter the user id to whom you want to recommend: ")
predicted_movies = User_item_score1(int(user))
print(" ")
print("The Recommendations for User Id:")
print("   ")
for i in predicted_movies:
    print(i)
